{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro, vou organizar o código em funções, usar o algoritmo de Descida de Gradiente (Gradient Descent) para o treinamento e inicializar os parâmetros aleatoriamente. Neste exemplo, vamos usar um exemplo de treinamento com a tarefa de classificação binária (0 ou 1).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Função de ativação sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Inicialização aleatória de pesos e bias\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    hidden_weights = np.random.randn(input_size, hidden_size)\n",
    "    hidden_bias = np.zeros(hidden_size)\n",
    "    output_weights = np.random.randn(hidden_size, output_size)\n",
    "    output_bias = np.zeros(output_size)\n",
    "    return hidden_weights, hidden_bias, output_weights, output_bias\n",
    "\n",
    "# Forward Propagation\n",
    "def forward_propagation(X, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    hidden_input = np.dot(X, hidden_weights) + hidden_bias\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    output_input = np.dot(hidden_output, output_weights) + output_bias\n",
    "    predicted_output = sigmoid(output_input)\n",
    "    return hidden_output, predicted_output\n",
    "\n",
    "# Calcula a perda (erro) usando a função de perda de entropia cruzada\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Backpropagation e atualização dos parâmetros\n",
    "def backpropagation(X, y_true, hidden_output, predicted_output, hidden_weights, output_weights, learning_rate):\n",
    "    output_error = predicted_output - y_true\n",
    "    output_delta = output_error * (predicted_output * (1 - predicted_output))\n",
    "    hidden_error = np.dot(output_delta, output_weights.T)\n",
    "    hidden_delta = hidden_error * (hidden_output * (1 - hidden_output))\n",
    "    \n",
    "    output_weights -= learning_rate * np.dot(hidden_output.T, output_delta)\n",
    "    output_bias -= learning_rate * np.sum(output_delta, axis=0)\n",
    "    hidden_weights -= learning_rate * np.dot(X.T, hidden_delta)\n",
    "    hidden_bias -= learning_rate * np.sum(hidden_delta, axis=0)\n",
    "\n",
    "# Treinamento da rede neural usando Gradient Descent\n",
    "def train_neural_network(X, y, hidden_size, learning_rate, num_epochs):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    \n",
    "    hidden_weights, hidden_bias, output_weights, output_bias = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hidden_output, predicted_output = forward_propagation(X, hidden_weights, hidden_bias, output_weights, output_bias)\n",
    "        loss = compute_loss(y, predicted_output)\n",
    "        backpropagation(X, y, hidden_output, predicted_output, hidden_weights, output_weights, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return hidden_weights, hidden_bias, output_weights, output_bias\n",
    "\n",
    "# Dados de entrada (exemplo de treinamento)\n",
    "X = np.array([[0.5, 0.2]])\n",
    "y = np.array([[1]])\n",
    "\n",
    "# Parâmetros do treinamento\n",
    "hidden_size = 3\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "\n",
    "# Treinamento da rede neural\n",
    "trained_hidden_weights, trained_hidden_bias, trained_output_weights, trained_output_bias = train_neural_network(X, y, hidden_size, learning_rate, num_epochs)\n",
    "\n",
    "# Avaliação da rede neural\n",
    "hidden_output, predicted_output = forward_propagation(X, trained_hidden_weights, trained_hidden_bias, trained_output_weights, trained_output_bias)\n",
    "print(\"Entrada:\", X)\n",
    "print(\"Saída prevista:\", predicted_output)\n",
    "```\n",
    "\n",
    "Agora, vamos explicar as etapas:\n",
    "\n",
    "1. **Função de Ativação Sigmoid:**\n",
    "   A função sigmoid é utilizada como função de ativação para introduzir não-linearidade na rede.\n",
    "\n",
    "2. **Inicialização Aleatória de Parâmetros:**\n",
    "   A função `initialize_parameters()` inicializa os pesos e bias das camadas oculta e de saída aleatoriamente.\n",
    "\n",
    "3. **Forward Propagation:**\n",
    "   A função `forward_propagation()` realiza a propagação direta da entrada através da rede neural, calculando as saídas das camadas oculta e de saída.\n",
    "\n",
    "4. **Cálculo da Perda:**\n",
    "   A função `compute_loss()` calcula a perda (erro) usando a função de perda de entropia cruzada.\n",
    "\n",
    "5. **Backpropagation e Atualização de Parâmetros:**\n",
    "   A função `backpropagation()` calcula os gradientes para cada camada e atualiza os parâmetros usando o Gradient Descent.\n",
    "\n",
    "6. **Treinamento da Rede Neural:**\n",
    "   A função `train_neural_network()` executa o treinamento da rede neural por um número específico de épocas. Ela chama as funções de forward e backward propagation para cada época.\n",
    "\n",
    "7. **Dados de Entrada e Parâmetros do Treinamento:**\n",
    "   Os dados de entrada (um exemplo de treinamento) e os parâmetros do treinamento (tamanho da camada oculta, taxa de aprendizado e número de épocas) são definidos.\n",
    "\n",
    "8. **Treinamento e Avaliação:**\n",
    "   A rede neural é treinada e avaliada usando os dados de entrada. A saída prevista é impressa.\n",
    "\n",
    "Lembre-se de que esse é um exemplo simples e didático. Em cenários reais, é necessário um conjunto de dados maior e um algoritmo de otimização mais avançado para obter melhores resultados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dois\n",
    "\n",
    "Claro! Vou criar um exemplo de código dividido em funções que usa uma rede neural com uma quantidade variável de camadas ocultas para resolver um problema de classificação multiclasse. Vamos usar a biblioteca NumPy para as operações matriciais.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Função de ativação sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Inicialização aleatória de pesos e bias\n",
    "def initialize_parameters(layer_sizes):\n",
    "    parameters = {}\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1])\n",
    "        parameters[f'b{i}'] = np.zeros((layer_sizes[i], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# Forward Propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    cache = {'A0': X}\n",
    "    num_layers = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        Z = np.dot(W, cache[f'A{i-1}']) + b\n",
    "        A = sigmoid(Z)\n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "    \n",
    "    return cache[f'A{num_layers-1}'], cache\n",
    "\n",
    "# Calcula a perda usando a função de perda de entropia cruzada\n",
    "def compute_loss(Y, Y_pred):\n",
    "    m = Y.shape[1]\n",
    "    loss = - (1 / m) * np.sum(Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "    return loss\n",
    "\n",
    "# Backpropagation e atualização dos parâmetros\n",
    "def backpropagation(X, Y, cache, parameters, learning_rate):\n",
    "    grads = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dA = - (Y / cache[f'A{num_layers-1}']) + (1 - Y) / (1 - cache[f'A{num_layers-1}'])\n",
    "    \n",
    "    for i in range(num_layers, 0, -1):\n",
    "        dZ = dA * (cache[f'A{i}'] * (1 - cache[f'A{i}']))\n",
    "        dW = (1 / m) * np.dot(dZ, cache[f'A{i-1}'].T)\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA = np.dot(parameters[f'W{i}'].T, dZ)\n",
    "        \n",
    "        grads[f'dW{i}'] = dW\n",
    "        grads[f'db{i}'] = db\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        parameters[f'W{i}'] -= learning_rate * grads[f'dW{i}']\n",
    "        parameters[f'b{i}'] -= learning_rate * grads[f'db{i}']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Treinamento da rede neural\n",
    "def train_neural_network(X, Y, layer_sizes, learning_rate, num_epochs):\n",
    "    parameters = initialize_parameters(layer_sizes)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        Y_pred, cache = forward_propagation(X, parameters)\n",
    "        loss = compute_loss(Y, Y_pred)\n",
    "        parameters = backpropagation(X, Y, cache, parameters, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Dados de entrada (exemplo de treinamento)\n",
    "X = np.array([[0.5, 0.2], [0.7, 0.3]])\n",
    "Y = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# Parâmetros do treinamento\n",
    "layer_sizes = [2, 4, 3, 2]  # Camadas: entrada - 4 - 3 - saída (duas camadas ocultas)\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "\n",
    "# Treinamento da rede neural\n",
    "trained_parameters = train_neural_network(X, Y, layer_sizes, learning_rate, num_epochs)\n",
    "\n",
    "# Avaliação da rede neural\n",
    "Y_pred, _ = forward_propagation(X, trained_parameters)\n",
    "print(\"Entrada:\", X)\n",
    "print(\"Saída prevista:\", Y_pred)\n",
    "```\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- A função `initialize_parameters()` inicializa os pesos e bias de todas as camadas, com base nas dimensões especificadas em `layer_sizes`.\n",
    "\n",
    "- A função `forward_propagation()` realiza a propagação direta da entrada através de todas as camadas ocultas até a camada de saída.\n",
    "\n",
    "- A função `compute_loss()` calcula a perda (erro) usando a função de perda de entropia cruzada.\n",
    "\n",
    "- A função `backpropagation()` calcula os gradientes usando o algoritmo de retropropagação e atualiza os parâmetros usando o Gradient Descent.\n",
    "\n",
    "- A função `train_neural_network()` treina a rede neural por um número específico de épocas, chamando as funções de propagação direta e retropropagação.\n",
    "\n",
    "- Os dados de entrada `X` e os rótulos `Y` são definidos.\n",
    "\n",
    "- Os parâmetros do treinamento, como as dimensões das camadas, a taxa de aprendizado e o número de épocas, são especificados.\n",
    "\n",
    "- A rede neural é treinada e avaliada, e a saída prevista é impressa.\n",
    "\n",
    "Lembre-se de que este é um exemplo simplificado para fins didáticos. Em problemas reais, é importante ajustar os hiperparâmetros, usar conjuntos de dados maiores e aplicar técnicas de validação cruzada para avaliar a performance da rede."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro! Vou criar um exemplo de código detalhado em funções que implementa um modelo de regressão linear usando o método dos mínimos quadrados. Vamos usar a biblioteca NumPy para as operações matriciais.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Função para ajustar o modelo de regressão linear\n",
    "def fit_linear_regression(X, y):\n",
    "    X_b = np.c_[X, np.ones((X.shape[0], 1))]  # Adiciona uma coluna de 1's para representar o termo de bias\n",
    "    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y  # Fórmula dos mínimos quadrados\n",
    "    return theta\n",
    "\n",
    "# Função para fazer previsões usando o modelo de regressão linear\n",
    "def predict_linear_regression(X, theta):\n",
    "    X_b = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    y_pred = X_b @ theta\n",
    "    return y_pred\n",
    "\n",
    "# Dados de entrada (exemplo de treinamento)\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([[2], [4], [5], [4], [5]])\n",
    "\n",
    "# Ajustar o modelo de regressão linear\n",
    "theta = fit_linear_regression(X, y)\n",
    "\n",
    "# Fazer previsões usando o modelo\n",
    "X_test = np.array([[6], [7]])\n",
    "y_pred = predict_linear_regression(X_test, theta)\n",
    "\n",
    "print(\"Coeficientes theta:\", theta)\n",
    "print(\"Previsões para X_test:\", y_pred)\n",
    "```\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- A função `fit_linear_regression()` ajusta um modelo de regressão linear aos dados de entrada `X` e rótulos `y`. Ela adiciona uma coluna de 1's em `X` para representar o termo de bias e calcula os coeficientes theta usando a fórmula dos mínimos quadrados.\n",
    "\n",
    "- A função `predict_linear_regression()` faz previsões usando o modelo de regressão linear ajustado. Ela também adiciona uma coluna de 1's em `X` e multiplica pela matriz de coeficientes theta para obter as previsões.\n",
    "\n",
    "- Os dados de entrada `X` e os rótulos `y` são definidos.\n",
    "\n",
    "- Ajustamos o modelo de regressão linear usando os dados de treinamento.\n",
    "\n",
    "- Fazemos previsões usando o modelo ajustado para os dados de teste `X_test`.\n",
    "\n",
    "- Imprimimos os coeficientes theta do modelo ajustado e as previsões para `X_test`.\n",
    "\n",
    "Lembre-se de que este é um exemplo básico para ilustrar o conceito de regressão linear. Em cenários reais, é importante avaliar a performance do modelo usando métricas adequadas, fazer uma validação cruzada e considerar técnicas de regularização para evitar overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro! Vou criar um exemplo de código em que usamos a regularização L2 (também conhecida como regularização de peso ao quadrado) em uma rede neural. Vamos usar funções separadas para cada etapa e explicar detalhadamente cada parte.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Função de ativação sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Inicialização aleatória de pesos e bias\n",
    "def initialize_parameters(layer_sizes):\n",
    "    parameters = {}\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1])\n",
    "        parameters[f'b{i}'] = np.zeros((layer_sizes[i], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# Forward Propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    cache = {'A0': X}\n",
    "    num_layers = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        Z = np.dot(W, cache[f'A{i-1}']) + b\n",
    "        A = sigmoid(Z)\n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "    \n",
    "    return cache[f'A{num_layers-1}'], cache\n",
    "\n",
    "# Calcula a perda usando a função de perda de entropia cruzada + L2 regularization\n",
    "def compute_loss_with_regularization(Y, Y_pred, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    cross_entropy_loss = - (1 / m) * np.sum(Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "    \n",
    "    regularization_term = 0\n",
    "    num_layers = len(parameters) // 2\n",
    "    for i in range(1, num_layers):\n",
    "        W = parameters[f'W{i}']\n",
    "        regularization_term += np.sum(np.square(W))\n",
    "    \n",
    "    loss = cross_entropy_loss + (lambd / (2 * m)) * regularization_term\n",
    "    return loss\n",
    "\n",
    "# Backpropagation e atualização dos parâmetros com L2 regularization\n",
    "def backpropagation(X, Y, cache, parameters, learning_rate, lambd):\n",
    "    grads = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dA = - (Y / cache[f'A{num_layers-1}']) + (1 - Y) / (1 - cache[f'A{num_layers-1}'])\n",
    "    \n",
    "    for i in range(num_layers, 0, -1):\n",
    "        dZ = dA * (cache[f'A{i}'] * (1 - cache[f'A{i}']))\n",
    "        dW = (1 / m) * np.dot(dZ, cache[f'A{i-1}'].T) + (lambd / m) * parameters[f'W{i}']\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA = np.dot(parameters[f'W{i}'].T, dZ)\n",
    "        \n",
    "        grads[f'dW{i}'] = dW\n",
    "        grads[f'db{i}'] = db\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        parameters[f'W{i}'] -= learning_rate * grads[f'dW{i}']\n",
    "        parameters[f'b{i}'] -= learning_rate * grads[f'db{i}']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Treinamento da rede neural com L2 regularization\n",
    "def train_neural_network_with_regularization(X, Y, layer_sizes, learning_rate, lambd, num_epochs):\n",
    "    parameters = initialize_parameters(layer_sizes)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        Y_pred, cache = forward_propagation(X, parameters)\n",
    "        loss = compute_loss_with_regularization(Y, Y_pred, parameters, lambd)\n",
    "        parameters = backpropagation(X, Y, cache, parameters, learning_rate, lambd)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Dados de entrada (exemplo de treinamento)\n",
    "X = np.array([[0.5, 0.2], [0.7, 0.3]])\n",
    "Y = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# Parâmetros do treinamento\n",
    "layer_sizes = [2, 4, 3, 2]  # Camadas: entrada - 4 - 3 - saída (duas camadas ocultas)\n",
    "learning_rate = 0.1\n",
    "lambd = 0.01  # Fator de regularização L2\n",
    "num_epochs = 1000\n",
    "\n",
    "# Treinamento da rede neural com L2 regularization\n",
    "trained_parameters = train_neural_network_with_regularization(X, Y, layer_sizes, learning_rate, lambd, num_epochs)\n",
    "\n",
    "# Avaliação da rede neural\n",
    "Y_pred, _ = forward_propagation(X, trained_parameters)\n",
    "print(\"Entrada:\", X)\n",
    "print(\"Saída prevista:\", Y_pred)\n",
    "```\n",
    "\n",
    "Aqui está uma explicação detalhada do código:\n",
    "\n",
    "1. **Inicialização de Parâmetros:** A função `initialize_parameters()` inicializa os pesos e bias de todas as camadas da mesma forma que antes.\n",
    "\n",
    "2. **Forward Propagation:** A função `forward_propagation()` realiza a propagação direta da entrada através de todas as camadas ocultas até a camada de saída, igualmente ao exemplo anterior.\n",
    "\n",
    "3. **Cálculo da Perda com Regularização L2:** A função `compute_loss_with_regularization()` calcula a perda usando a função de perda de entropia cruzada, mas também adiciona a regularização L2 ao cálculo da perda.\n",
    "\n",
    "4. **Backpropagation e Atualização com Regularização L2:** A função `backpropagation()` calcula os gradientes, mas agora também considera a regularização L2 ao calcular os gradientes dos pesos.\n",
    "\n",
    "5. **Treinamento da Rede Neural com Regularização L2:** A função `train_neural_network_with_regularization()` executa o treinamento da rede neural, mas também inclui o fator de regularização L2.\n",
    "\n",
    "6. **Parâmetro de Regularização L2:** O parâmetro `lambd` é o fator de regularização L2 que controla a intensidade da regularização.\n",
    "\n",
    "O restante do código é semelhante ao exemplo anterior. Note que a regularização L2 ajuda a evitar overfitting penalizando os pesos maiores e incentivando-os a serem mais próximos de zero. Isso ajuda a tornar o modelo mais geral e a melhorar sua capacidade de generalização para novos dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro! Aqui está um exemplo de código que implementa uma rede neural com dropout, usando funções separadas para cada etapa e explicando detalhadamente cada parte.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Função de ativação sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Inicialização aleatória de pesos e bias\n",
    "def initialize_parameters(layer_sizes):\n",
    "    parameters = {}\n",
    "    num_layers = len(layer_sizes)\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1])\n",
    "        parameters[f'b{i}'] = np.zeros((layer_sizes[i], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# Implementação do dropout durante a forward propagation\n",
    "def forward_propagation_with_dropout(X, parameters, keep_prob):\n",
    "    cache = {'A0': X}\n",
    "    num_layers = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        Z = np.dot(W, cache[f'A{i-1}']) + b\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "        # Aplicação do dropout\n",
    "        if i < num_layers - 1:\n",
    "            D = np.random.rand(A.shape[0], A.shape[1]) < keep_prob\n",
    "            A = np.multiply(A, D) / keep_prob\n",
    "            cache[f'D{i}'] = D\n",
    "        \n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "    \n",
    "    return cache[f'A{num_layers-1}'], cache\n",
    "\n",
    "# Função de custo\n",
    "def compute_loss(Y, Y_pred):\n",
    "    m = Y.shape[1]\n",
    "    loss = - (1 / m) * np.sum(Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "    return loss\n",
    "\n",
    "# Backpropagation e atualização dos parâmetros\n",
    "def backpropagation(X, Y, cache, parameters, learning_rate, keep_prob):\n",
    "    grads = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dA = - (Y / cache[f'A{num_layers-1}']) + (1 - Y) / (1 - cache[f'A{num_layers-1}'])\n",
    "    \n",
    "    for i in range(num_layers, 0, -1):\n",
    "        if i < num_layers:\n",
    "            dA = np.multiply(dA, cache[f'D{i}']) / keep_prob\n",
    "        \n",
    "        dZ = dA * (cache[f'A{i}'] * (1 - cache[f'A{i}']))\n",
    "        dW = (1 / m) * np.dot(dZ, cache[f'A{i-1}'].T)\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA = np.dot(parameters[f'W{i}'].T, dZ)\n",
    "        \n",
    "        grads[f'dW{i}'] = dW\n",
    "        grads[f'db{i}'] = db\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        parameters[f'W{i}'] -= learning_rate * grads[f'dW{i}']\n",
    "        parameters[f'b{i}'] -= learning_rate * grads[f'db{i}']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Treinamento da rede neural com dropout\n",
    "def train_neural_network_with_dropout(X, Y, layer_sizes, learning_rate, keep_prob, num_epochs):\n",
    "    parameters = initialize_parameters(layer_sizes)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        Y_pred, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        loss = compute_loss(Y, Y_pred)\n",
    "        parameters = backpropagation(X, Y, cache, parameters, learning_rate, keep_prob)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Dados de entrada (exemplo de treinamento)\n",
    "X = np.array([[0.5, 0.2], [0.7, 0.3]])\n",
    "Y = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# Parâmetros do treinamento\n",
    "layer_sizes = [2, 4, 3, 2]  # Camadas: entrada - 4 - 3 - saída (duas camadas ocultas)\n",
    "learning_rate = 0.1\n",
    "keep_prob = 0.8  # Probabilidade de manter um neurônio ativo durante o dropout\n",
    "num_epochs = 1000\n",
    "\n",
    "# Treinamento da rede neural com dropout\n",
    "trained_parameters = train_neural_network_with_dropout(X, Y, layer_sizes, learning_rate, keep_prob, num_epochs)\n",
    "\n",
    "# Avaliação da rede neural\n",
    "Y_pred, _ = forward_propagation_with_dropout(X, trained_parameters, keep_prob)\n",
    "print(\"Entrada:\", X)\n",
    "print(\"Saída prevista:\", Y_pred)\n",
    "```\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- A função `forward_propagation_with_dropout()` realiza a propagação direta da entrada através de todas as camadas ocultas, aplicando o dropout apenas nas camadas ocultas. O dropout é aplicado aleatoriamente a neurônios durante a propagação direta.\n",
    "\n",
    "- A função `backpropagation()` considera o dropout ao propagar os gradientes de volta pela rede. Os gradientes são multiplicados pelo valor de dropout e normalizados pela probabilidade de manter um neurônio ativo durante o dropout.\n",
    "\n",
    "- O parâmetro `keep_prob` controla a probabilidade de manter um neurônio ativo durante o dropout. Um valor típico é 0.8, o que significa que cada neurônio tem 80% de probabilidade de ser mantido ativo durante o dropout.\n",
    "\n",
    "O uso de dropout ajuda a prevenir o overfitting, desativando aleatoriamente neurônios durante o treinamento, o que torna a rede mais robusta e reduz a dependência de neurônios específicos. Isso resulta em uma melhor generalização para novos dados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
