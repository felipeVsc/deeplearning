{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Add bias term to feature matrix\n",
    "X_train_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# One-hot encode the target labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_encoded = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Initialize weights\n",
    "num_features = X_train_bias.shape[1]\n",
    "num_classes = y_train_encoded.shape[1]\n",
    "weights = np.random.randn(num_features, num_classes)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Calculate scores\n",
    "    scores = np.dot(X_train_bias, weights)\n",
    "    \n",
    "    # Compute softmax probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute gradients\n",
    "    error = probs - y_train_encoded\n",
    "    gradient = np.dot(X_train_bias.T, error) / X_train_bias.shape[0]\n",
    "    \n",
    "    # Update weights\n",
    "    weights -= learning_rate * gradient\n",
    "\n",
    "# Make predictions\n",
    "scores = np.dot(X_test_bias, weights)\n",
    "predicted_labels = np.argmax(scores, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient_descent(X, y, num_iterations, learning_rate):\n",
    "    num_samples, num_features = X.shape\n",
    "    weights = np.zeros((num_features, 1))\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        scores = np.dot(X, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error) / num_samples\n",
    "        weights -= learning_rate * gradient\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def one_vs_rest_train(X_train, y_train, num_classes, num_iterations, learning_rate):\n",
    "    weights_list = []\n",
    "    \n",
    "    for class_label in range(num_classes):\n",
    "        y_class = (y_train == class_label).astype(int).reshape(-1, 1)\n",
    "        weights = gradient_descent(X_train, y_class, num_iterations, learning_rate)\n",
    "        weights_list.append(weights)\n",
    "    \n",
    "    return weights_list\n",
    "\n",
    "def one_vs_rest_predict(X_test, weights_list):\n",
    "    scores = np.zeros((X_test.shape[0], len(weights_list)))\n",
    "    \n",
    "    for idx, weights in enumerate(weights_list):\n",
    "        scores[:, idx] = np.dot(X_test, weights).flatten()  # Flatten the scores\n",
    "    predicted_labels = np.argmax(scores, axis=1)\n",
    "    return predicted_labels\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Add bias term to feature matrix\n",
    "X_train_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# Train OvR logistic regression models\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "num_classes = len(np.unique(y))\n",
    "weights_list = one_vs_rest_train(X_train_bias, y_train, num_classes, num_iterations, learning_rate)\n",
    "\n",
    "# Make predictions\n",
    "predicted_labels = one_vs_rest_predict(X_test_bias, weights_list)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset (3 classes)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Add a bias term to X\n",
    "X_biased = np.c_[X, np.ones(X.shape[0])]\n",
    "num_features = X_biased.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "y_onehot = np.eye(num_classes)[y]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_biased, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.randn(num_features, num_classes)\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    logits = np.dot(X_train, weights)\n",
    "    probabilities = softmax(logits)\n",
    "\n",
    "    # Compute gradients\n",
    "    error = probabilities - y_train\n",
    "    gradients = np.dot(X_train.T, error)\n",
    "\n",
    "    # Update weights\n",
    "    weights -= learning_rate * gradients\n",
    "\n",
    "# Testing\n",
    "logits_test = np.dot(X_test, weights)\n",
    "probabilities_test = softmax(logits_test)\n",
    "predicted_classes = np.argmax(probabilities_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predicted_classes == np.argmax(y_test, axis=1))\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
