
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/preprocessing/plot_target_encoder_cross_val.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_preprocessing_plot_target_encoder_cross_val.py>`
        to download the full example code or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_preprocessing_plot_target_encoder_cross_val.py:


=======================================
Target Encoder's Internal Cross fitting
=======================================

.. currentmodule:: sklearn.preprocessing

The :class:`TargetEnocoder` replaces each category of a categorical feature with
the mean of the target variable for that category. This method is useful
in cases where there is a strong relationship between the categorical feature
and the target. To prevent overfitting, :meth:`TargetEncoder.fit_transform` uses
an internal cross fitting scheme to encode the training data to be used by a
downstream model. In this example, we demonstrate the importance of the cross fitting
procedure to prevent overfitting.

.. GENERATED FROM PYTHON SOURCE LINES 18-24

Create Synthetic Dataset
========================
For this example, we build a dataset with three categorical features: an informative
feature with medium cardinality, an uninformative feature with medium cardinality,
and an uninformative feature with high cardinality. First, we generate the informative
feature:

.. GENERATED FROM PYTHON SOURCE LINES 24-45

.. code-block:: default

    import numpy as np

    from sklearn.preprocessing import KBinsDiscretizer

    n_samples = 50_000

    rng = np.random.RandomState(42)
    y = rng.randn(n_samples)
    noise = 0.5 * rng.randn(n_samples)
    n_categories = 100

    kbins = KBinsDiscretizer(
        n_bins=n_categories, encode="ordinal", strategy="uniform", random_state=rng
    )
    X_informative = kbins.fit_transform((y + noise).reshape(-1, 1))

    # Remove the linear relationship between y and the bin index by permuting the values of
    # X_informative
    permuted_categories = rng.permutation(n_categories)
    X_informative = permuted_categories[X_informative.astype(np.int32)]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/circleci/project/sklearn/preprocessing/_discretization.py:239: FutureWarning:

    In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.





.. GENERATED FROM PYTHON SOURCE LINES 46-48

The uninformative feature with medium cardinality is generated by permuting the
informative feature and removing the relationship with the target:

.. GENERATED FROM PYTHON SOURCE LINES 48-50

.. code-block:: default

    X_shuffled = rng.permutation(X_informative)








.. GENERATED FROM PYTHON SOURCE LINES 51-58

The uninformative feature with high cardinality is generated so that is independent of
the target variable. We will show that target encoding without cross fitting will
cause catastrophic overfitting for the downstream regressor. These high cardinality
features are basically unique identifiers for samples which should generally be
removed from machine learning dataset. In this example, we generate them to show how
:class:`TargetEncoder`'s default cross fitting behavior mitigates the overfitting
issue automatically.

.. GENERATED FROM PYTHON SOURCE LINES 58-62

.. code-block:: default

    X_near_unique_categories = rng.choice(
        int(0.9 * n_samples), size=n_samples, replace=True
    ).reshape(-1, 1)








.. GENERATED FROM PYTHON SOURCE LINES 63-64

Finally, we assemble the dataset and perform a train test split:

.. GENERATED FROM PYTHON SOURCE LINES 64-77

.. code-block:: default

    import pandas as pd

    from sklearn.model_selection import train_test_split

    X = pd.DataFrame(
        np.concatenate(
            [X_informative, X_shuffled, X_near_unique_categories],
            axis=1,
        ),
        columns=["informative", "shuffled", "near_unique"],
    )
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)








.. GENERATED FROM PYTHON SOURCE LINES 78-85

Training a Ridge Regressor
==========================
In this section, we train a ridge regressor on the dataset with and without
encoding and explore the influence of target encoder with and without the
internal cross fitting. First, we see the Ridge model trained on the
raw features will have low performance, because the order of the informative
feature is not informative:

.. GENERATED FROM PYTHON SOURCE LINES 85-97

.. code-block:: default

    import sklearn
    from sklearn.linear_model import Ridge

    # Configure transformers to always output DataFrames
    sklearn.set_config(transform_output="pandas")

    ridge = Ridge(alpha=1e-6, solver="lsqr", fit_intercept=False)

    raw_model = ridge.fit(X_train, y_train)
    print("Raw Model score on training set: ", raw_model.score(X_train, y_train))
    print("Raw Model score on test set: ", raw_model.score(X_test, y_test))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Raw Model score on training set:  0.0049896314219657345
    Raw Model score on test set:  0.00457762158159003




.. GENERATED FROM PYTHON SOURCE LINES 98-101

Next, we create a pipeline with the target encoder and ridge model. The pipeline
uses :meth:`TargetEncoder.fit_transform` which uses cross fitting. We see that
the model fits the data well and generalizes to the test set:

.. GENERATED FROM PYTHON SOURCE LINES 101-109

.. code-block:: default

    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import TargetEncoder

    model_with_cv = make_pipeline(TargetEncoder(random_state=0), ridge)
    model_with_cv.fit(X_train, y_train)
    print("Model with CV on training set: ", model_with_cv.score(X_train, y_train))
    print("Model with CV on test set: ", model_with_cv.score(X_test, y_test))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Model with CV on training set:  0.8000184677460285
    Model with CV on test set:  0.7927845601690948




.. GENERATED FROM PYTHON SOURCE LINES 110-112

The coefficients of the linear model shows that most of the weight is on the
feature at column index 0, which is the informative feature

.. GENERATED FROM PYTHON SOURCE LINES 112-122

.. code-block:: default

    import matplotlib.pyplot as plt
    import pandas as pd

    plt.rcParams["figure.constrained_layout.use"] = True

    coefs_cv = pd.Series(
        model_with_cv[-1].coef_, index=model_with_cv[-1].feature_names_in_
    ).sort_values()
    _ = coefs_cv.plot(kind="barh")




.. image-sg:: /auto_examples/preprocessing/images/sphx_glr_plot_target_encoder_cross_val_001.png
   :alt: plot target encoder cross val
   :srcset: /auto_examples/preprocessing/images/sphx_glr_plot_target_encoder_cross_val_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 123-129

While :meth:`TargetEncoder.fit_transform` uses an internal cross fitting scheme,
:meth:`TargetEncoder.transform` itself does not perform any cross fitting.
It uses the aggregation of the complete training set to transform the categorical
features. Thus, we can use :meth:`TargetEncoder.fit` followed by
:meth:`TargetEncoder.transform` to disable the cross fitting. This encoding
is then passed to the ridge model.

.. GENERATED FROM PYTHON SOURCE LINES 129-136

.. code-block:: default

    target_encoder = TargetEncoder(random_state=0)
    target_encoder.fit(X_train, y_train)
    X_train_no_cv_encoding = target_encoder.transform(X_train)
    X_test_no_cv_encoding = target_encoder.transform(X_test)

    model_no_cv = ridge.fit(X_train_no_cv_encoding, y_train)








.. GENERATED FROM PYTHON SOURCE LINES 137-138

We evaluate the model on the non-cross validated encoding and see that it overfits:

.. GENERATED FROM PYTHON SOURCE LINES 138-146

.. code-block:: default

    print(
        "Model without CV on training set: ",
        model_no_cv.score(X_train_no_cv_encoding, y_train),
    )
    print(
        "Model without CV on test set: ", model_no_cv.score(X_test_no_cv_encoding, y_test)
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Model without CV on training set:  0.858486250088675
    Model without CV on test set:  0.6338211367110066




.. GENERATED FROM PYTHON SOURCE LINES 147-149

The ridge model overfits, because it assigns more weight to the extremely high
cardinality feature relative to the informative feature.

.. GENERATED FROM PYTHON SOURCE LINES 149-154

.. code-block:: default

    coefs_no_cv = pd.Series(
        model_no_cv.coef_, index=model_no_cv.feature_names_in_
    ).sort_values()
    _ = coefs_no_cv.plot(kind="barh")




.. image-sg:: /auto_examples/preprocessing/images/sphx_glr_plot_target_encoder_cross_val_002.png
   :alt: plot target encoder cross val
   :srcset: /auto_examples/preprocessing/images/sphx_glr_plot_target_encoder_cross_val_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 155-163

Conclusion
==========
This example demonstrates the importance of :class:`TargetEncoder`'s internal cross
fitting. It is important to use :meth:`TargetEncoder.fit_transform` to encode
training data before passing it to a machine learning model. When a
:class:`TargetEncoder` is a part of a :class:`~sklearn.pipeline.Pipeline` and the
pipeline is fitted, the pipeline will correctly call
:meth:`TargetEncoder.fit_transform` and pass the encoding along.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.265 seconds)


.. _sphx_glr_download_auto_examples_preprocessing_plot_target_encoder_cross_val.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/1.3.X?urlpath=lab/tree/notebooks/auto_examples/preprocessing/plot_target_encoder_cross_val.ipynb
        :alt: Launch binder
        :width: 150 px



    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../../lite/lab/?path=auto_examples/preprocessing/plot_target_encoder_cross_val.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_target_encoder_cross_val.py <plot_target_encoder_cross_val.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_target_encoder_cross_val.ipynb <plot_target_encoder_cross_val.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
